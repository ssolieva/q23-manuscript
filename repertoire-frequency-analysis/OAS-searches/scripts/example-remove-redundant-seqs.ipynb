{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfbbfd95",
   "metadata": {},
   "source": [
    "        06102024\n",
    "        Remove redunant sequences (15%/85%) in PG9. \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a81d0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26b13659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: load results. \n",
    "def load_results(which_search): \n",
    "    '''\n",
    "    input: all_log.txt \n",
    "    output: parsed all_log: donor_names, cdrh3_seqs_by_donor\n",
    "    '''\n",
    "    logfile_path = f\"all_log.txt\"  #print(f\">Loading in search results for: {which_search}\")\n",
    "    \n",
    "    # STEP 1: open and read the log file which contains the search results:\n",
    "    with open(logfile_path, \"r\") as fd: # import the results file \n",
    "        logfile = fd.read().splitlines() # read in the results\n",
    "        logfile_split = [] \n",
    "        for line in logfile:\n",
    "            logfile_split.append(line.split()) # save each line to the list logfile_split\n",
    "    fd.close()\n",
    "    \n",
    "    # STEP 2: save out the donor names and cdrh3 seqs: (full seqs not needed right now)\n",
    "    donor_names = []\n",
    "    cdrh3_seqs  = []\n",
    "    full_seqs   = []\n",
    "    dgene = []\n",
    "    for line in range(len(logfile_split)): # for each line \n",
    "        for entry in range(len(logfile_split[line])): # for each entry in each line\n",
    "            if logfile_split[line][entry].rfind(\"IGHV\") == 0: # find which entry the Vgene is at \n",
    "                dgene.append(logfile_split[line][entry+1]) # save the d gene \n",
    "                if 'csv' in logfile_split[line][entry-1]: # if a .csv file is right before the Vgene, \n",
    "                    donor_names.append(\"no_donor_name_found\") # then this line has no donor name\n",
    "                else: # if the .csv line is not right before the Vgene, \n",
    "                    donor_names.append(logfile_split[line][entry-1]) # then save the reported donor name  \n",
    "            if logfile_split[line][entry].isdigit() == True:\n",
    "                if int(logfile_split[line][entry]) == len(logfile_split[line][entry-1]):\n",
    "                    cdrh3_seqs.append(logfile_split[line][entry-1])     \n",
    "                \n",
    "        full_seqs.append(logfile_split[line][-2])\n",
    "    \n",
    "    \n",
    "    #print(len(dgene), len(cdrh3_seqs), len(donor_names))\n",
    "    \n",
    "    updated_donor_names = []\n",
    "    updated_cdrh3_seqs = []\n",
    "    updated_dgene = []\n",
    "    updated_full_seqs = [] \n",
    "    for i in range(len(donor_names)):\n",
    "        if donor_names[i] != 'no_donor_name_found' and donor_names[i] != 'no':\n",
    "            updated_donor_names.append(donor_names[i])\n",
    "            updated_cdrh3_seqs.append(cdrh3_seqs[i])\n",
    "            updated_dgene.append(dgene[i])\n",
    "            updated_full_seqs.append(full_seqs[i])\n",
    "            \n",
    "    # STEP 3: save / group sequences according to donor names\n",
    "    unique_donor_names = np.unique(updated_donor_names)\n",
    "    cdrh3_seqs_by_donor = []\n",
    "    full_seqs_by_donor = []\n",
    "    for i in range(len(unique_donor_names)):\n",
    "        tmp_cdrh3 = []\n",
    "        tmp_fullseq = []\n",
    "        for j in range(len(donor_names)):\n",
    "            tmp_tmp_cdrh3 = []\n",
    "            tmp_tmp_fullseq = []\n",
    "            if donor_names[j] == unique_donor_names[i] and donor_names[j] != 'no' and donor_names[j] != 'no_donor_name_found':\n",
    "                tmp_tmp_cdrh3.append(cdrh3_seqs[j])\n",
    "                tmp_tmp_fullseq.append(full_seqs[j])\n",
    "            tmp_cdrh3.append(tmp_tmp_cdrh3)\n",
    "            tmp_fullseq.append(tmp_tmp_fullseq)\n",
    "        cdrh3_seqs_by_donor.append(np.concatenate(tmp_cdrh3))\n",
    "        full_seqs_by_donor.append(np.concatenate(tmp_fullseq))\n",
    "    return updated_donor_names, cdrh3_seqs_by_donor, full_seqs_by_donor, updated_cdrh3_seqs, updated_dgene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d104df23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ssolieva/Desktop/Kulp_lab/projects/OAS_database_searches/PG9/PG9_noVgene_cdrh3_anylen_motif_4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd405f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "which_search = 'PG9_noVgene_cdrh3_anylen_motif_4'\n",
    "donor_names, cdrh3_seqs_by_donor, full_seqs_by_donor, cdrh3_seqs, dgenes = load_results(which_search) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cfde807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 152 292238 292238\n"
     ]
    }
   ],
   "source": [
    "print(len(cdrh3_seqs_by_donor), len(full_seqs_by_donor), len(donor_names), len(cdrh3_seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d669b8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cdrh3_seqs_by_donor_ind = []\n",
    "for i in range(len(cdrh3_seqs_by_donor)):\n",
    "    unique_cdrh3_seqs_by_donor_ind.append(np.unique(cdrh3_seqs_by_donor[i], return_index=True))\n",
    "\n",
    "unique_full_seq_by_donor = []\n",
    "for i in range(len(full_seqs_by_donor)):\n",
    "    unique_full_seq_by_donor0 = []\n",
    "    for j in range(len(unique_cdrh3_seqs_by_donor_ind[i][1])):\n",
    "        #print(i,j, unique_cdrh3_seqs_by_donor_ind[i][1][j])\n",
    "        n = unique_cdrh3_seqs_by_donor_ind[i][1][j]\n",
    "        unique_full_seq_by_donor0.append(full_seqs_by_donor[i][n])\n",
    "    unique_full_seq_by_donor.append(unique_full_seq_by_donor0)\n",
    "\n",
    "def diff_letters(a,b):\n",
    "    '''reports the number of different letters (residues)'''\n",
    "    return sum ( a[i] != b[i] for i in range(len(a)) )\n",
    "\n",
    "    \n",
    "def remove_almost_duplicates(dataset_cdrh3, dataset_full_seq):\n",
    "    '''\n",
    "    data = unique_cdrh3_seqs_by_donor[0]\n",
    "    '''\n",
    "    dataset = dataset_cdrh3\n",
    "    \n",
    "    different_seqs_final      = [] # final cdrh3 sequences\n",
    "    different_seqs_final_full = [] # final full sequences\n",
    "    \n",
    "    # get unique lengths:\n",
    "    unique_lengths = []\n",
    "    for seq in range(len(dataset)):\n",
    "        unique_lengths.append(len(dataset[seq]))\n",
    "    unique_lengths = np.unique(unique_lengths)\n",
    "    #print(unique_lengths)\n",
    "    \n",
    "    # group data into unique length groups:\n",
    "    unique_length_groups = []\n",
    "    unique_length_groups_full = []\n",
    "    for length in range(len(unique_lengths)):\n",
    "        unique_length_groups_0 = []\n",
    "        unique_length_groups_0_full = []\n",
    "        for seq in range(len(dataset)):\n",
    "            unique_length_groups_00 = []\n",
    "            unique_length_groups_00_full = []\n",
    "            if len(dataset[seq]) == unique_lengths[length]:\n",
    "                unique_length_groups_00.append(dataset[seq])\n",
    "                unique_length_groups_00_full.append(dataset_full_seq[seq])\n",
    "            unique_length_groups_0.append(unique_length_groups_00)\n",
    "            unique_length_groups_0_full.append(unique_length_groups_00_full)\n",
    "        unique_length_groups.append(np.concatenate(unique_length_groups_0))\n",
    "        unique_length_groups_full.append(np.concatenate(unique_length_groups_0_full))\n",
    "    #print(d, len(unique_length_groups), unique_lengths)\n",
    "    \n",
    "    # for each unique length group, get rid of duplicates and almost duplicates\n",
    "    for group in range(len(unique_length_groups)):\n",
    "        if len(unique_length_groups[group]) == 1: # if there's only 1 sequence, then save that sequence\n",
    "            #print(len(unique_length_groups[group]), unique_length_groups[group][0])\n",
    "            different_seqs_final.append(unique_length_groups[group][0])\n",
    "            different_seqs_final_full.append(unique_length_groups_full[group][0])\n",
    "        \n",
    "        all_seqs_updated = []\n",
    "        all_seqs_updated_full = [] # new line\n",
    "        \n",
    "        if len(unique_length_groups[group]) > 1: # if more than 1 sequence, \n",
    "            all_seqs = unique_length_groups[group]\n",
    "            all_seqs_full = unique_length_groups_full[group] #unique_length_groups[group] # new line\n",
    "            \n",
    "            a = unique_length_groups[group]\n",
    "            b = unique_length_groups[group]\n",
    "            pairs = [(i, j) for i in a for j in b if i < j] # make pairwise pairs of the sequences\n",
    "            #print('number of pairs:', len(pairs))\n",
    "            \n",
    "            for pair in range(len(pairs)):\n",
    "                n_diff = diff_letters(pairs[pair][0], pairs[pair][1])\n",
    "                if n_diff <= (int(len(pairs[pair][0])*0.15)):                 # 15% / 85%\n",
    "                   # print(n_diff, pairs[pair][0], pairs[pair][1])\n",
    "                    result = np.where(all_seqs==f'{pairs[pair][1]}')[0]\n",
    "                    all_seqs = np.delete(all_seqs, result) \n",
    "                    all_seqs_full =  np.delete(all_seqs_full, result) \n",
    "            for al in range(len(all_seqs)):\n",
    "                all_seqs_updated.append(all_seqs[al])\n",
    "                all_seqs_updated_full.append(all_seqs_full[al])\n",
    "            \n",
    "        #print('all_seqs_updated', all_seqs_updated[0],  all_seqs_updated_full[0])\n",
    "        #print(all_seqs_updated, len(all_seqs_updated))\n",
    "        if len(all_seqs_updated) != 0:\n",
    "            for ds in range(len(all_seqs_updated)):\n",
    "                different_seqs_final.append(all_seqs_updated[ds])\n",
    "                different_seqs_final_full.append(all_seqs_updated_full[ds])\n",
    "                \n",
    "    return different_seqs_final, different_seqs_final_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fab7ad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_exact_cdrh3_duplicates_within_donors(full_seqs_by_donor, cdrh3_seqs_by_donor):\n",
    "    print('full sequences')\n",
    "    unique_full_seqs_by_donor = []\n",
    "    for i in range(len(full_seqs_by_donor)):\n",
    "        unique_full_seqs_by_donor.append(np.unique(full_seqs_by_donor[i]))\n",
    "    print(f'\\tnumber of duplicate full sequences: {len(np.concatenate(full_seqs_by_donor)) - len(np.concatenate(unique_full_seqs_by_donor))}')\n",
    "    print(f'\\tn_seq after full sequence duplicates within each donor removed: {len(np.concatenate(unique_full_seqs_by_donor))}')\n",
    "    print('cdrh3 sequences')\n",
    "    unique_cdrh3_seqs_by_donor = []\n",
    "    for i in range(len(cdrh3_seqs_by_donor)):\n",
    "        unique_cdrh3_seqs_by_donor.append(np.unique(cdrh3_seqs_by_donor[i]))\n",
    "    print(f'\\tnumber of duplicate CDRH3 sequences: {len(np.concatenate(full_seqs_by_donor))- len(np.concatenate(unique_cdrh3_seqs_by_donor))}')\n",
    "    print(f'\\tnumber of sequences after removing exact CDRH3 duplicate sequences: {len(np.concatenate(unique_cdrh3_seqs_by_donor))}')\n",
    "    return unique_full_seqs_by_donor, unique_cdrh3_seqs_by_donor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a02fe61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full sequences\n",
      "\tnumber of duplicate full sequences: 26668\n",
      "\tn_seq after full sequence duplicates within each donor removed: 265570\n",
      "cdrh3 sequences\n",
      "\tnumber of duplicate CDRH3 sequences: 209770\n",
      "\tnumber of sequences after removing exact CDRH3 duplicate sequences: 82468\n"
     ]
    }
   ],
   "source": [
    "unique_full_seqs_by_donor, unique_cdrh3_seqs_by_donor = remove_exact_cdrh3_duplicates_within_donors(full_seqs_by_donor, cdrh3_seqs_by_donor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11f5b628",
   "metadata": {},
   "outputs": [],
   "source": [
    "different_seqs_final_all = []\n",
    "different_seqs_final_full_all = []\n",
    "\n",
    "for donor in range(len(unique_cdrh3_seqs_by_donor)):\n",
    "    different_seqs_final,different_seqs_final_full = remove_almost_duplicates(unique_cdrh3_seqs_by_donor[donor], unique_full_seq_by_donor[donor])\n",
    "    different_seqs_final_all.append(different_seqs_final)\n",
    "    different_seqs_final_full_all.append(different_seqs_final_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e850e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "different_seqs_final_full_all_concat = np.concatenate(different_seqs_final_full_all)\n",
    "np.save(\"PG9_noVgene_cdrh3_anylen_motif_4__full_seqs.npy\", different_seqs_final_full_all_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a519c5f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38427"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(different_seqs_final_full_all_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5f83ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ssolieva/Desktop/Kulp_lab/projects/OAS_database_searches/PG9/PG9_noVgene_cdrh3_anylen_motif_4'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4b403b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1921.35"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(different_seqs_final_full_all_concat)/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f815249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
